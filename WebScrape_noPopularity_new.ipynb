{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:26.276593Z",
     "start_time": "2018-02-05T08:36:25.869247Z"
    }
   },
   "outputs": [],
   "source": [
    "#from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:26.282321Z",
     "start_time": "2018-02-05T08:36:26.279306Z"
    }
   },
   "outputs": [],
   "source": [
    "my_url = 'https://www.amazon.com/Fire-Fury-Inside-Trump-White/' +\\\n",
    "         'product-reviews/1250158060/ref=cm_cr_getr_d_paging_btm_1' +\\\n",
    "         '?ie=UTF8&reviewerType=all_reviews&pageNumber=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:26.630289Z",
     "start_time": "2018-02-05T08:36:26.284823Z"
    }
   },
   "outputs": [],
   "source": [
    "page = requests.get(my_url)\n",
    "html_contents = page.text\n",
    "#html_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:26.819665Z",
     "start_time": "2018-02-05T08:36:26.633065Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_contents, \"html.parser\")\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vote"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T07:10:52.247645Z",
     "start_time": "2018-02-05T07:10:52.203300Z"
    },
    "scrolled": true
   },
   "source": [
    "votes = soup.find_all('span', 'review-votes')\n",
    "print (len(votes))\n",
    "#votes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T07:10:52.266853Z",
     "start_time": "2018-02-05T07:10:52.249942Z"
    }
   },
   "source": [
    "vot_list = []\n",
    "for i in range(len(votes)-2):\n",
    "    m = votes[i+2].text\n",
    "    like_num = m.lstrip('\\n ').rstrip(' people found this helpful.\\n ')\n",
    "    like_num = int(float(like_num.replace(\",\",\"\")))\n",
    "    vot_list.append(like_num)\n",
    "vot_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T07:10:52.311434Z",
     "start_time": "2018-02-05T07:10:52.269037Z"
    }
   },
   "source": [
    "def get_votes():\n",
    "    votes = soup.find_all('span', 'review-votes')\n",
    "    vot_list = []\n",
    "    if len(votes) == 10:\n",
    "        for i in range(len(votes)):\n",
    "            m = votes[i].text\n",
    "            like_num = m.lstrip('\\n ').rstrip(' people found this helpful.\\n ')\n",
    "            try:\n",
    "                like_num = int(float(like_num.replace(\",\",\"\")))\n",
    "            except ValueError:\n",
    "                like_num = 1\n",
    "            vot_list.append(like_num)\n",
    "        return vot_list\n",
    "    elif len(votes) == 12:\n",
    "        for i in range(len(votes)-2):\n",
    "            m = votes[i+2].text\n",
    "            like_num = m.lstrip('\\n ').rstrip(' people found this helpful.\\n ')\n",
    "            try:\n",
    "                like_num = int(float(like_num.replace(\",\",\"\")))\n",
    "            except ValueError:\n",
    "                like_num = 1\n",
    "            vot_list.append(like_num)\n",
    "        return vot_list\n",
    "    else:\n",
    "        vot_list = [0,0,0,0,0,0,0,0,0,0]\n",
    "        print(\"votes data are wrong\")\n",
    "        return vot_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Title"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T01:00:01.139893Z",
     "start_time": "2018-02-05T01:00:01.089082Z"
    }
   },
   "source": [
    "#the first two duplicate reviews\n",
    "title_top = soup.find_all('span', 'a-size-base review-title a-text-bold')\n",
    "print (len(title_top))\n",
    "title_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:26.861265Z",
     "start_time": "2018-02-05T08:36:26.821545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "titles = soup.find_all('a', 'a-size-base a-link-normal review-title a-color-base a-text-bold')\n",
    "print (len(titles))\n",
    "#titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:26.880214Z",
     "start_time": "2018-02-05T08:36:26.863942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reading this book is like being forced to eat an entire 5 gallon carton of ice cream',\n",
       " 'I got the kindle version this morning. I probably ...',\n",
       " 'I am not a phony billionaire. I am a ...',\n",
       " 'Fascinating Look Into the Trump Presidency',\n",
       " 'Before listening to the talking heads on Fox/CNN, read it',\n",
       " 'Entertaining trash',\n",
       " \"Everything you thought you knew...wasn't bad enough.\",\n",
       " 'Read It & Weep if you Hate It & Weep if you Like It. But do read it.',\n",
       " 'Wow...',\n",
       " 'Excellent and Fun Read']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tit_list = []\n",
    "for i in range(len(titles)):\n",
    "    title = titles[i].text\n",
    "    tit_list.append(title)\n",
    "tit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:26.891863Z",
     "start_time": "2018-02-05T08:36:26.882921Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_titles():\n",
    "    titles = soup.find_all('a', 'a-size-base a-link-normal review-title a-color-base a-text-bold')\n",
    "    tit_list = []\n",
    "    for i in range(len(titles)):\n",
    "        title = titles[i].text\n",
    "        tit_list.append(title)\n",
    "    return tit_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:26.940232Z",
     "start_time": "2018-02-05T08:36:26.894124Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "dates = soup.find_all('span', 'a-size-base a-color-secondary review-date')\n",
    "print (len(dates))\n",
    "#dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:26.953363Z",
     "start_time": "2018-02-05T08:36:26.942581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['January-5-2018',\n",
       " 'January-5-2018',\n",
       " 'January-5-2018',\n",
       " 'January-8-2018',\n",
       " 'January-8-2018',\n",
       " 'January-14-2018',\n",
       " 'January-5-2018',\n",
       " 'January-22-2018',\n",
       " 'January-7-2018',\n",
       " 'January-6-2018']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_list = []\n",
    "for i in range(len(dates)-2):\n",
    "    m = dates[i+2].text\n",
    "    date = m.lstrip('on ')\n",
    "    date = date.replace(\",\",\"\").replace(\" \",\"-\")\n",
    "    dat_list.append(date)\n",
    "dat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:26.969228Z",
     "start_time": "2018-02-05T08:36:26.956560Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dates():\n",
    "    dates = soup.find_all('span', 'a-size-base a-color-secondary review-date')\n",
    "    dat_list = []\n",
    "    for i in range(len(dates)-2):\n",
    "        m = dates[i+2].text\n",
    "        date = m.lstrip('on ')\n",
    "        date = date.replace(\",\",\"\").replace(\" \",\"-\")\n",
    "        dat_list.append(date)\n",
    "    return dat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Auther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:27.015334Z",
     "start_time": "2018-02-05T08:36:26.971477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "authers = soup.find_all('a', 'a-size-base a-link-normal author')\n",
    "print (len(authers))\n",
    "#authers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:27.025695Z",
     "start_time": "2018-02-05T08:36:27.018056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Smooth Sluggo',\n",
       " 'J. Thornton',\n",
       " 'Victoria Earle',\n",
       " 'smiley edna',\n",
       " 'Thomas Manes',\n",
       " 'G. L Vince',\n",
       " 'J.L. Cartwright',\n",
       " 'Rabid Reader',\n",
       " 'Aaron Olson',\n",
       " 'Angela Stevens']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aut_list = []\n",
    "for i in range(len(authers)):\n",
    "    auther = authers[i].text\n",
    "    aut_list.append(auther)\n",
    "aut_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:27.035926Z",
     "start_time": "2018-02-05T08:36:27.028239Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_authers():\n",
    "    authers = soup.find_all('a', 'a-size-base a-link-normal author')\n",
    "    aut_list = []\n",
    "    for i in range(len(authers)):\n",
    "        auther = authers[i].text\n",
    "        aut_list.append(auther)\n",
    "    return aut_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:27.089967Z",
     "start_time": "2018-02-05T08:36:27.038105Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<span class=\"a-icon-alt\">4.4 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">5.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">3.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">5.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">5.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">5.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">4.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">5.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">3.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">5.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">4.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">5.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">5.0 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">|</span>,\n",
       " <span class=\"a-icon-alt\">3.3 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">4.5 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">4.2 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">2.9 out of 5 stars</span>,\n",
       " <span class=\"a-icon-alt\">4.5 out of 5 stars</span>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#starting from the 4th valid star, to the end before the last 5.\n",
    "stars = soup.find_all('span', 'a-icon-alt')\n",
    "print (len(stars))\n",
    "stars"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T07:10:52.614736Z",
     "start_time": "2018-02-05T07:10:52.596851Z"
    }
   },
   "source": [
    "if len(stars) > 34:\n",
    "    sta_list = []\n",
    "    for i in range(len(stars)-29):\n",
    "        m = stars[(i+1)*3+1].text\n",
    "        star = m.rstrip(' out of 5 stars')\n",
    "        star = float(star)\n",
    "        sta_list.append(star)\n",
    "    sta_list\n",
    "else:\n",
    "    sta_list = []\n",
    "    for i in range(len(stars)-24):\n",
    "        m = stars[(i+1)*3+1].text\n",
    "        star = m.rstrip(' out of 5 stars')\n",
    "        star = float(star)\n",
    "        sta_list.append(star)\n",
    "    sta_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T06:52:51.481681Z",
     "start_time": "2018-02-05T06:52:51.449898Z"
    }
   },
   "source": [
    "def get_stars():\n",
    "    stars = soup.find_all('span', 'a-icon-alt')\n",
    "    if len(stars) > 34:\n",
    "        sta_list = []\n",
    "        for i in range(len(stars)-29):\n",
    "            m = stars[(i+1)*3+1].text\n",
    "            star = m.rstrip(' out of 5 stars')\n",
    "            star = float(star)\n",
    "            sta_list.append(star)\n",
    "        return sta_list\n",
    "    else:\n",
    "        sta_list = []\n",
    "        for i in range(len(stars)-24):\n",
    "            m = stars[(i+1)*3+1].text\n",
    "            star = m.rstrip(' out of 5 stars')\n",
    "            star = float(star)\n",
    "            sta_list.append(star)\n",
    "        return sta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:27.114291Z",
     "start_time": "2018-02-05T08:36:27.092400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 out of 5 stars\n",
      "|\n",
      "|\n",
      "5.0 out of 5 stars\n",
      "|\n",
      "|\n",
      "5.0 out of 5 stars\n",
      "|\n",
      "|\n",
      "4.0 out of 5 stars\n",
      "|\n",
      "|\n",
      "5.0 out of 5 stars\n",
      "|\n",
      "|\n",
      "3.0 out of 5 stars\n",
      "|\n",
      "|\n",
      "5.0 out of 5 stars\n",
      "|\n",
      "|\n",
      "4.0 out of 5 stars\n",
      "|\n",
      "|\n",
      "5.0 out of 5 stars\n",
      "|\n",
      "|\n",
      "5.0 out of 5 stars\n",
      "|\n",
      "|\n",
      "3.3 out of 5 stars\n",
      "4.5 out of 5 stars\n",
      "4.2 out of 5 stars\n",
      "2.9 out of 5 stars\n",
      "4.5 out of 5 stars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.0, 5.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 5.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sta_list = []\n",
    "for i in range(4,len(stars)):\n",
    "    m = stars[i].text\n",
    "    print(m)\n",
    "    if m == \"|\":\n",
    "        pass\n",
    "    else:\n",
    "    #print(i, (i+1)*3+1)\n",
    "        star = m.rstrip(' out of 5 stars')\n",
    "        star = float(star)\n",
    "        sta_list.append(star)\n",
    "sta_list = sta_list[:-5]\n",
    "sta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:27.132415Z",
     "start_time": "2018-02-05T08:36:27.117397Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stars():\n",
    "    stars = soup.find_all('span', 'a-icon-alt')\n",
    "    print(len(stars))\n",
    "    sta_list = []\n",
    "    for i in range(4,len(stars)):\n",
    "        m = stars[i].text\n",
    "        #print(m)\n",
    "        if m == \"|\":\n",
    "            pass\n",
    "        else:\n",
    "            star = m.rstrip(' out of 5 stars')\n",
    "            star = float(star)\n",
    "            sta_list.append(star)\n",
    "    sta_list = sta_list[:-5]\n",
    "    return sta_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:27.185739Z",
     "start_time": "2018-02-05T08:36:27.134878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "review_body = soup.find_all('span', 'a-size-base review-text')\n",
    "print (len(review_body))\n",
    "#review_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:27.197213Z",
     "start_time": "2018-02-05T08:36:27.188046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What can be said, that hasn't already been said about the President?Nothing in this book will surprise anyone who has been following the trials and tribulations of the 45th President of the United States, Donald J. Trump.  His supporters will cry foul, and say some of it isn't true -- and some of it might not be -- but if even 10% of what is in this book is true, holy crap!Reading this book is like being forced to eat an entire 5 gallon carton of ice cream in one sitting.  It is yummy and you don't want to stop.  And then you do want to stop but you can't, because the book has you roped in.  You should feel good, but in reality you feel awful, because of what you just did to yourself.So tread lightly, dear reader.  You won't want to put the book down, but you won't be better off for reading it, either.  If you don't like it, do something productive -- stop eating the ice cream and get off the couch.  Make America AMERICA again!\",\n",
       " \"I got the kindle version this morning. I probably would not have bought this. I will probably see most of this book for free as the media tears into it over the next few weeks. My decision to purchase was based solely on Trump's furious efforts to keep it from coming out. It was well worth 15$ to do my part to keep this at the top of the best-sellers list.\",\n",
       " \"I am not a phony billionaire.  I am a genuine hourly worker who can afford $15 to show Trump that he can't order a book out of publication in America.  Also, I'm a voter.\",\n",
       " \"Fire and Fury, Inside the Trump White House is actually a pretty good book. Contrary to popular -- obviously unread --opinions, it's not just one big Trumpbash. It does highlight an incoming administration that was unprepared for the task at hand and it does go into some sordid details that were already known about the president, it doesn't spend a lot of time speculating about the Russian dossier or offer any validity into the Russian collusion investigation, it does highlight how the press and the intelligence communities can use this sort of information and ongoing investigations to destabilize a presidency. There is a great deal of historical precedence presented of how political players (aka 'deep state') have used real and/or imagined scandals to destabilize and control a regime. Trump has no real political allies and plenty of enemies.\",\n",
       " \"I'm on page 240, and I have to say. It is a good read so far. I have yet to find anything that I already didn't know, however the author stitches all of the information together with the timeline of events. Adding in the information he gathered from time spent in the west wing and on the trail to the White House.I recommend to anyone, read it. Before listening to the talking heads on Fox/CNN, read it. Then spend a few minutes watching the talking heads, its amazing the bits each channel seems to focus on. Spin, Spin, Spin from all sides.\",\n",
       " 'As a card carrying Never Trumper, \"Fire and Fury\" is very entertaining but I find Wolff\\'s credibility to be suspect at best. The \"Game of Thrones\" type battle between Bannon, Priebus and Kushner for Trump\\'s ear is believable and there is plenty of Bannon dirt in here. The rest just doesn\\'t seem to be backed up with much. That being said, if you can get past any issues with Wolff\\'s reputation and the numerous errors, it\\'s not a bad book. Just comes off as more of a trashy beach read than your typical political journalism.',\n",
       " 'This book doesn\\'t wander far from what my expectations of this administration is up to. It\\'s utterly gross, and yet, like the proverbial train wreck, you CANNOT look away. I read the entire thing this morning, and I\\'m going to have to go back and read it again. To be sure that I actually read all the )#&%# I thought I did, and did such #(%&*# actually make it into a work about how the administration is going about its daily business.This is a constitutional crisis. I have read, already, all the attacks on the author. But everything he says shores up my POV about the administration - they were not prepared, they are a bumbling roadside attraction doing anything they can to keep the audience\\'s attention. When whatever the current shill is turns out not to be what they want you to see, they bumble and fall over one another, inflaming hatred of some \"other\" - from the top down - to shift your attention.I have been watching, enjoying, and debating politics since I was ten. I love to see what current administrations are doing, and even if I voted for them, I\\'ll critique. No one\\'s perfect. But this - good grief. I have been ashamed, from time to time, at some of the actions of past presidents. But I am constantly, 150%, 24/7 ashamed of this administration. There is nothing they do that doesn\\'t hurt, lash out, or inflict damage on some part of the American people. Outside of that group the pres wants to be part of, the billionaires, everyone else is merely fodder.And lord help you if you are anyone other than white and male. Because you\\'re nothing.All this book has done is reinforce what I thought I was seeing. While that makes me sad, angry, and worried - it also reinforces that we must be active, and not complacent - we must, as a nation, go vote for people who will support this nation, and the ideals it has survived on. We have to bring back people who are not the sellout pieces of trash that is our current legislative bodies.Most of all, this book is a warning. Our nation is under siege. If you don\\'t see it - it\\'s because you don\\'t want to, or feel that the siege will benefit you in some fashion. It won\\'t. If it does, it\\'s temporary, and you will pay the piper, and it will hurt. This book should show you that. The pres has loyalty and concern for one person - himself. Anyone else is disposable. That includes the nation he supposedly represents.I suggest you read it. And put down the partisanship, and read it with eyes of \"Is this who I want to represent me?\" Even if only half of this is true, we are in a world of hurt. WE deserve better. The pres and all his sycophantic horde deserve to be bounced out on their backsides. Which begins with the 2018 elections.',\n",
       " \"Clearly, if you are a Trump enthusiast, you might not like this. Read it anyhow.If you are anti-Trump in your politics? Read it.If you are generally bored by politics? Read it anyhow.If you're a political junkie? For sure read this.This is partly so that you can have some inkling of what all the fuss is about, and also so you can get a--and this is qualified--peek inside a government run by flawed people.All governments are run by flawed people, as we know too well. But this government is perhaps exceptional in that respect. . .Politics aside, this is an easy, and fairly quick read.I was amused to see that Wolff seems to have a pretty deft hand with political and other slang--which made the book feel chatty to some extent. Certainly it made it accessible. A bit like Stephen King's neighborly word-style.What to say about the content?Well, IF Wolff really did sit around the White House interviewing people willy-nilly as he says he did--as members of the Administration confirm he did--then it's even more important to read the book. If not, well, then I'd say it was still an interesting read and a down-to-earth sort of filled-in timeline for what's been happening (hirings, firings, general political sashaying and so forth) written in a fairly painless form.\",\n",
       " 'Very interesting read, recapping many things I knew, but giving a clear understanding of how this White House runs...fascinating to read.',\n",
       " 'Although there are apparently inaccuracies in the book, to me the bulk of everything I read seems obviously real. This is an extraordinarily well written book, and very enjoyable to read as it seems like a fiction, but it’s not... it’s fascinating history we are living through.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bod_list = []\n",
    "for i in range(len(review_body)):\n",
    "    body = review_body[i].text\n",
    "    #body = m.rstrip(' out of 5 stars')\n",
    "    bod_list.append(body)\n",
    "bod_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:27.210909Z",
     "start_time": "2018-02-05T08:36:27.201651Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_revbody():\n",
    "    review_body = soup.find_all('span', 'a-size-base review-text') \n",
    "    bod_list = []\n",
    "    for i in range(len(review_body)):\n",
    "        body = review_body[i].text\n",
    "        #body = m.rstrip(' out of 5 stars')\n",
    "        bod_list.append(body)\n",
    "    return bod_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:32:23.188192Z",
     "start_time": "2018-02-05T08:32:23.068821Z"
    },
    "scrolled": true
   },
   "source": [
    "shoppinglist = [aut_list, sta_list, dat_list, tit_list, bod_list, vot_list]\n",
    "for i, name in enumerate(shoppinglist):\n",
    "    print (len(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:27.239787Z",
     "start_time": "2018-02-05T08:36:27.213213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What can be said, that hasn't already been sai...</td>\n",
       "      <td>January-5-2018</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Smooth Sluggo</td>\n",
       "      <td>Reading this book is like being forced to eat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I got the kindle version this morning. I proba...</td>\n",
       "      <td>January-5-2018</td>\n",
       "      <td>5.0</td>\n",
       "      <td>J. Thornton</td>\n",
       "      <td>I got the kindle version this morning. I proba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am not a phony billionaire.  I am a genuine ...</td>\n",
       "      <td>January-5-2018</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Victoria Earle</td>\n",
       "      <td>I am not a phony billionaire. I am a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fire and Fury, Inside the Trump White House is...</td>\n",
       "      <td>January-8-2018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>smiley edna</td>\n",
       "      <td>Fascinating Look Into the Trump Presidency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm on page 240, and I have to say. It is a go...</td>\n",
       "      <td>January-8-2018</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Thomas Manes</td>\n",
       "      <td>Before listening to the talking heads on Fox/C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content            date  rating  \\\n",
       "0  What can be said, that hasn't already been sai...  January-5-2018     5.0   \n",
       "1  I got the kindle version this morning. I proba...  January-5-2018     5.0   \n",
       "2  I am not a phony billionaire.  I am a genuine ...  January-5-2018     5.0   \n",
       "3  Fire and Fury, Inside the Trump White House is...  January-8-2018     4.0   \n",
       "4  I'm on page 240, and I have to say. It is a go...  January-8-2018     5.0   \n",
       "\n",
       "         reviewer                                              title  \n",
       "0   Smooth Sluggo  Reading this book is like being forced to eat ...  \n",
       "1     J. Thornton  I got the kindle version this morning. I proba...  \n",
       "2  Victoria Earle           I am not a phony billionaire. I am a ...  \n",
       "3     smiley edna         Fascinating Look Into the Trump Presidency  \n",
       "4    Thomas Manes  Before listening to the talking heads on Fox/C...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onepage = {'reviewer': aut_list,\n",
    "           'rating': sta_list,\n",
    "           'date': dat_list,\n",
    "           'title': tit_list,\n",
    "           'content': bod_list}\n",
    "df = pd.DataFrame.from_dict(onepage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:46:00.477381Z",
     "start_time": "2018-02-05T08:36:27.242603Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "get page 2, length of df: 20\n",
      "39\n",
      "get page 3, length of df: 30\n",
      "39\n",
      "get page 4, length of df: 40\n",
      "39\n",
      "get page 5, length of df: 50\n",
      "39\n",
      "get page 6, length of df: 60\n",
      "39\n",
      "get page 7, length of df: 70\n",
      "39\n",
      "get page 8, length of df: 80\n",
      "39\n",
      "get page 9, length of df: 90\n",
      "39\n",
      "get page 10, length of df: 100\n",
      "39\n",
      "get page 11, length of df: 110\n",
      "39\n",
      "get page 12, length of df: 120\n",
      "39\n",
      "get page 13, length of df: 130\n",
      "39\n",
      "get page 14, length of df: 140\n",
      "39\n",
      "get page 15, length of df: 150\n",
      "39\n",
      "get page 16, length of df: 160\n",
      "39\n",
      "get page 17, length of df: 170\n",
      "39\n",
      "get page 18, length of df: 180\n",
      "39\n",
      "get page 19, length of df: 190\n",
      "39\n",
      "get page 20, length of df: 200\n",
      "39\n",
      "get page 21, length of df: 210\n",
      "39\n",
      "get page 22, length of df: 220\n",
      "39\n",
      "get page 23, length of df: 230\n",
      "0\n",
      "get page 24, length of df: 230\n",
      "39\n",
      "get page 25, length of df: 240\n",
      "39\n",
      "get page 26, length of df: 250\n",
      "39\n",
      "get page 27, length of df: 260\n",
      "39\n",
      "get page 28, length of df: 270\n",
      "39\n",
      "get page 29, length of df: 280\n",
      "39\n",
      "get page 30, length of df: 290\n",
      "39\n",
      "get page 31, length of df: 300\n",
      "39\n",
      "get page 32, length of df: 310\n",
      "39\n",
      "get page 33, length of df: 320\n",
      "39\n",
      "get page 34, length of df: 330\n",
      "39\n",
      "get page 35, length of df: 340\n",
      "39\n",
      "get page 36, length of df: 350\n",
      "39\n",
      "get page 37, length of df: 360\n",
      "39\n",
      "get page 38, length of df: 370\n",
      "39\n",
      "get page 39, length of df: 380\n",
      "39\n",
      "get page 40, length of df: 390\n",
      "39\n",
      "get page 41, length of df: 400\n",
      "39\n",
      "get page 42, length of df: 410\n",
      "39\n",
      "get page 43, length of df: 420\n",
      "39\n",
      "get page 44, length of df: 430\n",
      "39\n",
      "get page 45, length of df: 440\n",
      "39\n",
      "get page 46, length of df: 450\n",
      "39\n",
      "get page 47, length of df: 460\n",
      "39\n",
      "get page 48, length of df: 470\n",
      "39\n",
      "get page 49, length of df: 480\n",
      "39\n",
      "get page 50, length of df: 490\n",
      "39\n",
      "get page 51, length of df: 500\n",
      "39\n",
      "get page 52, length of df: 510\n",
      "39\n",
      "get page 53, length of df: 520\n",
      "39\n",
      "get page 54, length of df: 530\n",
      "39\n",
      "get page 55, length of df: 540\n",
      "39\n",
      "get page 56, length of df: 550\n",
      "39\n",
      "get page 57, length of df: 560\n",
      "39\n",
      "get page 58, length of df: 570\n",
      "39\n",
      "get page 59, length of df: 580\n",
      "39\n",
      "get page 60, length of df: 590\n",
      "39\n",
      "get page 61, length of df: 600\n",
      "0\n",
      "get page 62, length of df: 600\n",
      "39\n",
      "get page 63, length of df: 610\n",
      "39\n",
      "get page 64, length of df: 620\n",
      "39\n",
      "get page 65, length of df: 630\n",
      "39\n",
      "get page 66, length of df: 640\n",
      "39\n",
      "get page 67, length of df: 650\n",
      "39\n",
      "get page 68, length of df: 660\n",
      "39\n",
      "get page 69, length of df: 670\n",
      "39\n",
      "get page 70, length of df: 680\n",
      "39\n",
      "get page 71, length of df: 690\n",
      "39\n",
      "get page 72, length of df: 700\n",
      "39\n",
      "get page 73, length of df: 710\n",
      "39\n",
      "get page 74, length of df: 720\n",
      "39\n",
      "get page 75, length of df: 730\n",
      "39\n",
      "get page 76, length of df: 740\n",
      "39\n",
      "get page 77, length of df: 750\n",
      "0\n",
      "get page 78, length of df: 750\n",
      "39\n",
      "get page 79, length of df: 760\n",
      "39\n",
      "get page 80, length of df: 770\n",
      "39\n",
      "get page 81, length of df: 780\n",
      "39\n",
      "get page 82, length of df: 790\n",
      "39\n",
      "get page 83, length of df: 800\n",
      "39\n",
      "get page 84, length of df: 810\n",
      "39\n",
      "get page 85, length of df: 820\n",
      "39\n",
      "get page 86, length of df: 830\n",
      "39\n",
      "get page 87, length of df: 840\n",
      "39\n",
      "get page 88, length of df: 850\n",
      "39\n",
      "get page 89, length of df: 860\n",
      "39\n",
      "get page 90, length of df: 870\n",
      "0\n",
      "get page 91, length of df: 870\n",
      "39\n",
      "get page 92, length of df: 880\n",
      "39\n",
      "get page 93, length of df: 890\n",
      "39\n",
      "get page 94, length of df: 900\n",
      "39\n",
      "get page 95, length of df: 910\n",
      "39\n",
      "get page 96, length of df: 920\n",
      "39\n",
      "get page 97, length of df: 930\n",
      "39\n",
      "get page 98, length of df: 940\n",
      "39\n",
      "get page 99, length of df: 950\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,100):\n",
    "    my_url = 'https://www.amazon.com/Fire-Fury-Inside-Trump-White/' +\\\n",
    "         'product-reviews/1250158060/ref=cm_cr_getr_d_paging_btm_' + str(i) +\\\n",
    "         '?ie=UTF8&reviewerType=all_reviews&pageNumber=' + str(i)\n",
    "    page = requests.get(my_url)\n",
    "\n",
    "    randtime = np.random.random(98) + np.random.randint(2,8,98)\n",
    "    time.sleep(randtime[i-2])\n",
    "    \n",
    "    html_contents = page.text\n",
    "    soup = BeautifulSoup(html_contents, \"html.parser\")   \n",
    "    \n",
    "    onepage = {'reviewer': get_authers(),\n",
    "           'rating': get_stars(),\n",
    "           'date': get_dates(),\n",
    "           'title': get_titles,\n",
    "           'content': get_revbody()}\n",
    "    try:\n",
    "        df_new = pd.DataFrame.from_dict(onepage)\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "        print(\"get page {}, length of df: {}\".format(i, len(df)))\n",
    "    except ValueError:\n",
    "        print (\"get page {}, arrays must all be same length, length of df: {}\".format(i, len(df)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:36:13.406924Z",
     "start_time": "2018-02-05T08:36:13.344657Z"
    }
   },
   "source": [
    "df.to_csv('partial_review_to100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T09:19:42.161711Z",
     "start_time": "2018-02-05T08:46:11.358191Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "get page 100, length of df: 960\n",
      "39\n",
      "get page 101, length of df: 970\n",
      "39\n",
      "get page 102, length of df: 980\n",
      "39\n",
      "get page 103, length of df: 990\n",
      "39\n",
      "get page 104, length of df: 1000\n",
      "39\n",
      "get page 105, length of df: 1010\n",
      "0\n",
      "get page 106, length of df: 1010\n",
      "39\n",
      "get page 107, length of df: 1020\n",
      "39\n",
      "get page 108, length of df: 1030\n",
      "39\n",
      "get page 109, length of df: 1040\n",
      "39\n",
      "get page 110, length of df: 1050\n",
      "39\n",
      "get page 111, length of df: 1060\n",
      "39\n",
      "get page 112, length of df: 1070\n",
      "39\n",
      "get page 113, length of df: 1080\n",
      "39\n",
      "get page 114, length of df: 1090\n",
      "39\n",
      "get page 115, length of df: 1100\n",
      "39\n",
      "get page 116, length of df: 1110\n",
      "39\n",
      "get page 117, length of df: 1120\n",
      "39\n",
      "get page 118, length of df: 1130\n",
      "39\n",
      "get page 119, length of df: 1140\n",
      "39\n",
      "get page 120, length of df: 1150\n",
      "39\n",
      "get page 121, length of df: 1160\n",
      "39\n",
      "get page 122, length of df: 1170\n",
      "39\n",
      "get page 123, length of df: 1180\n",
      "39\n",
      "get page 124, length of df: 1190\n",
      "39\n",
      "get page 125, length of df: 1200\n",
      "39\n",
      "get page 126, length of df: 1210\n",
      "39\n",
      "get page 127, length of df: 1220\n",
      "39\n",
      "get page 128, length of df: 1230\n",
      "39\n",
      "get page 129, length of df: 1240\n",
      "39\n",
      "get page 130, length of df: 1250\n",
      "39\n",
      "get page 131, length of df: 1260\n",
      "39\n",
      "get page 132, length of df: 1270\n",
      "39\n",
      "get page 133, length of df: 1280\n",
      "39\n",
      "get page 134, length of df: 1290\n",
      "39\n",
      "get page 135, length of df: 1300\n",
      "39\n",
      "get page 136, length of df: 1310\n",
      "39\n",
      "get page 137, length of df: 1320\n",
      "39\n",
      "get page 138, length of df: 1330\n",
      "39\n",
      "get page 139, length of df: 1340\n",
      "39\n",
      "get page 140, length of df: 1350\n",
      "39\n",
      "get page 141, length of df: 1360\n",
      "39\n",
      "get page 142, length of df: 1370\n",
      "39\n",
      "get page 143, length of df: 1380\n",
      "39\n",
      "get page 144, length of df: 1390\n",
      "39\n",
      "get page 145, length of df: 1400\n",
      "39\n",
      "get page 146, length of df: 1410\n",
      "39\n",
      "get page 147, length of df: 1420\n",
      "39\n",
      "get page 148, length of df: 1430\n",
      "39\n",
      "get page 149, length of df: 1440\n",
      "39\n",
      "get page 150, length of df: 1450\n",
      "39\n",
      "get page 151, length of df: 1460\n",
      "39\n",
      "get page 152, length of df: 1470\n",
      "38\n",
      "get page 153, length of df: 1480\n",
      "39\n",
      "get page 154, length of df: 1490\n",
      "39\n",
      "get page 155, length of df: 1500\n",
      "0\n",
      "get page 156, length of df: 1500\n",
      "39\n",
      "get page 157, length of df: 1510\n",
      "39\n",
      "get page 158, length of df: 1520\n",
      "39\n",
      "get page 159, length of df: 1530\n",
      "39\n",
      "get page 160, length of df: 1540\n",
      "38\n",
      "get page 161, length of df: 1550\n",
      "39\n",
      "get page 162, length of df: 1560\n",
      "39\n",
      "get page 163, length of df: 1570\n",
      "39\n",
      "get page 164, length of df: 1580\n",
      "0\n",
      "get page 165, length of df: 1580\n",
      "39\n",
      "get page 166, length of df: 1590\n",
      "39\n",
      "get page 167, length of df: 1600\n",
      "39\n",
      "get page 168, length of df: 1610\n",
      "38\n",
      "get page 169, length of df: 1620\n",
      "39\n",
      "get page 170, length of df: 1630\n",
      "0\n",
      "get page 171, length of df: 1630\n",
      "39\n",
      "get page 172, length of df: 1640\n",
      "39\n",
      "get page 173, length of df: 1650\n",
      "39\n",
      "get page 174, length of df: 1660\n",
      "39\n",
      "get page 175, length of df: 1670\n",
      "39\n",
      "get page 176, length of df: 1680\n",
      "39\n",
      "get page 177, length of df: 1690\n",
      "39\n",
      "get page 178, length of df: 1700\n",
      "39\n",
      "get page 179, length of df: 1710\n",
      "39\n",
      "get page 180, length of df: 1720\n",
      "34\n",
      "get page 181, arrays must all be same length, length of df: 1720\n",
      "39\n",
      "get page 182, length of df: 1730\n",
      "39\n",
      "get page 183, length of df: 1740\n",
      "39\n",
      "get page 184, length of df: 1750\n",
      "39\n",
      "get page 185, length of df: 1760\n",
      "39\n",
      "get page 186, length of df: 1770\n",
      "39\n",
      "get page 187, length of df: 1780\n",
      "39\n",
      "get page 188, length of df: 1790\n",
      "39\n",
      "get page 189, length of df: 1800\n",
      "39\n",
      "get page 190, length of df: 1810\n",
      "39\n",
      "get page 191, length of df: 1820\n",
      "39\n",
      "get page 192, length of df: 1830\n",
      "39\n",
      "get page 193, length of df: 1840\n",
      "39\n",
      "get page 194, length of df: 1850\n",
      "39\n",
      "get page 195, length of df: 1860\n",
      "39\n",
      "get page 196, length of df: 1870\n",
      "39\n",
      "get page 197, length of df: 1880\n",
      "39\n",
      "get page 198, length of df: 1890\n",
      "39\n",
      "get page 199, length of df: 1900\n",
      "39\n",
      "get page 200, length of df: 1910\n",
      "38\n",
      "get page 201, length of df: 1920\n",
      "38\n",
      "get page 202, length of df: 1930\n",
      "38\n",
      "get page 203, length of df: 1940\n",
      "39\n",
      "get page 204, length of df: 1950\n",
      "39\n",
      "get page 205, length of df: 1960\n",
      "39\n",
      "get page 206, length of df: 1970\n",
      "39\n",
      "get page 207, length of df: 1980\n",
      "39\n",
      "get page 208, length of df: 1990\n",
      "39\n",
      "get page 209, length of df: 2000\n",
      "39\n",
      "get page 210, length of df: 2010\n",
      "39\n",
      "get page 211, length of df: 2020\n",
      "39\n",
      "get page 212, length of df: 2030\n",
      "39\n",
      "get page 213, length of df: 2040\n",
      "39\n",
      "get page 214, length of df: 2050\n",
      "39\n",
      "get page 215, length of df: 2060\n",
      "39\n",
      "get page 216, length of df: 2070\n",
      "39\n",
      "get page 217, length of df: 2080\n",
      "39\n",
      "get page 218, length of df: 2090\n",
      "39\n",
      "get page 219, length of df: 2100\n",
      "39\n",
      "get page 220, length of df: 2110\n",
      "37\n",
      "get page 221, length of df: 2120\n",
      "39\n",
      "get page 222, length of df: 2130\n",
      "39\n",
      "get page 223, length of df: 2140\n",
      "39\n",
      "get page 224, length of df: 2150\n",
      "39\n",
      "get page 225, length of df: 2160\n",
      "38\n",
      "get page 226, length of df: 2170\n",
      "39\n",
      "get page 227, length of df: 2180\n",
      "39\n",
      "get page 228, length of df: 2190\n",
      "39\n",
      "get page 229, length of df: 2200\n",
      "39\n",
      "get page 230, length of df: 2210\n",
      "39\n",
      "get page 231, length of df: 2220\n",
      "39\n",
      "get page 232, length of df: 2230\n",
      "39\n",
      "get page 233, length of df: 2240\n",
      "38\n",
      "get page 234, length of df: 2250\n",
      "38\n",
      "get page 235, length of df: 2260\n",
      "38\n",
      "get page 236, length of df: 2270\n",
      "39\n",
      "get page 237, length of df: 2280\n",
      "0\n",
      "get page 238, length of df: 2280\n",
      "39\n",
      "get page 239, length of df: 2290\n",
      "39\n",
      "get page 240, length of df: 2300\n",
      "39\n",
      "get page 241, length of df: 2310\n",
      "39\n",
      "get page 242, length of df: 2320\n",
      "39\n",
      "get page 243, length of df: 2330\n",
      "39\n",
      "get page 244, length of df: 2340\n",
      "39\n",
      "get page 245, length of df: 2350\n",
      "39\n",
      "get page 246, length of df: 2360\n",
      "39\n",
      "get page 247, length of df: 2370\n",
      "38\n",
      "get page 248, length of df: 2380\n",
      "39\n",
      "get page 249, length of df: 2390\n",
      "39\n",
      "get page 250, length of df: 2400\n",
      "39\n",
      "get page 251, length of df: 2410\n",
      "39\n",
      "get page 252, length of df: 2420\n",
      "0\n",
      "get page 253, length of df: 2420\n",
      "39\n",
      "get page 254, length of df: 2430\n",
      "39\n",
      "get page 255, length of df: 2440\n",
      "39\n",
      "get page 256, length of df: 2450\n",
      "39\n",
      "get page 257, length of df: 2460\n",
      "39\n",
      "get page 258, length of df: 2470\n",
      "39\n",
      "get page 259, length of df: 2480\n",
      "39\n",
      "get page 260, length of df: 2490\n",
      "39\n",
      "get page 261, length of df: 2500\n",
      "39\n",
      "get page 262, length of df: 2510\n",
      "39\n",
      "get page 263, length of df: 2520\n",
      "38\n",
      "get page 264, length of df: 2530\n",
      "39\n",
      "get page 265, length of df: 2540\n",
      "38\n",
      "get page 266, length of df: 2550\n",
      "39\n",
      "get page 267, length of df: 2560\n",
      "39\n",
      "get page 268, length of df: 2570\n",
      "39\n",
      "get page 269, length of df: 2580\n",
      "39\n",
      "get page 270, length of df: 2590\n",
      "39\n",
      "get page 271, length of df: 2600\n",
      "38\n",
      "get page 272, length of df: 2610\n",
      "38\n",
      "get page 273, length of df: 2620\n",
      "39\n",
      "get page 274, length of df: 2630\n",
      "39\n",
      "get page 275, length of df: 2640\n",
      "39\n",
      "get page 276, length of df: 2650\n",
      "38\n",
      "get page 277, length of df: 2660\n",
      "39\n",
      "get page 278, length of df: 2670\n",
      "39\n",
      "get page 279, length of df: 2680\n",
      "39\n",
      "get page 280, length of df: 2690\n",
      "39\n",
      "get page 281, length of df: 2700\n",
      "38\n",
      "get page 282, length of df: 2710\n",
      "39\n",
      "get page 283, length of df: 2720\n",
      "38\n",
      "get page 284, length of df: 2730\n",
      "38\n",
      "get page 285, length of df: 2740\n",
      "39\n",
      "get page 286, length of df: 2750\n",
      "39\n",
      "get page 287, length of df: 2760\n",
      "38\n",
      "get page 288, length of df: 2770\n",
      "39\n",
      "get page 289, length of df: 2780\n",
      "39\n",
      "get page 290, length of df: 2790\n",
      "39\n",
      "get page 291, length of df: 2800\n",
      "39\n",
      "get page 292, length of df: 2810\n",
      "39\n",
      "get page 293, length of df: 2820\n",
      "39\n",
      "get page 294, length of df: 2830\n",
      "39\n",
      "get page 295, length of df: 2840\n",
      "39\n",
      "get page 296, length of df: 2850\n",
      "38\n",
      "get page 297, length of df: 2860\n",
      "37\n",
      "get page 298, length of df: 2870\n",
      "39\n",
      "get page 299, length of df: 2880\n",
      "39\n",
      "get page 300, length of df: 2890\n",
      "39\n",
      "get page 301, length of df: 2900\n",
      "39\n",
      "get page 302, length of df: 2910\n",
      "39\n",
      "get page 303, length of df: 2920\n",
      "39\n",
      "get page 304, length of df: 2930\n",
      "39\n",
      "get page 305, length of df: 2940\n",
      "39\n",
      "get page 306, length of df: 2950\n",
      "39\n",
      "get page 307, length of df: 2960\n",
      "39\n",
      "get page 308, length of df: 2970\n",
      "38\n",
      "get page 309, length of df: 2980\n",
      "39\n",
      "get page 310, length of df: 2990\n",
      "39\n",
      "get page 311, length of df: 3000\n",
      "39\n",
      "get page 312, length of df: 3010\n",
      "39\n",
      "get page 313, length of df: 3020\n",
      "39\n",
      "get page 314, length of df: 3030\n",
      "39\n",
      "get page 315, length of df: 3040\n",
      "39\n",
      "get page 316, length of df: 3050\n",
      "39\n",
      "get page 317, length of df: 3060\n",
      "39\n",
      "get page 318, length of df: 3070\n",
      "38\n",
      "get page 319, length of df: 3080\n",
      "39\n",
      "get page 320, length of df: 3090\n",
      "39\n",
      "get page 321, length of df: 3100\n",
      "38\n",
      "get page 322, length of df: 3110\n",
      "38\n",
      "get page 323, length of df: 3120\n",
      "38\n",
      "get page 324, length of df: 3130\n",
      "38\n",
      "get page 325, length of df: 3140\n",
      "38\n",
      "get page 326, length of df: 3150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "get page 327, length of df: 3160\n",
      "39\n",
      "get page 328, length of df: 3170\n",
      "39\n",
      "get page 329, length of df: 3180\n",
      "39\n",
      "get page 330, length of df: 3190\n",
      "39\n",
      "get page 331, length of df: 3200\n",
      "39\n",
      "get page 332, length of df: 3210\n",
      "39\n",
      "get page 333, length of df: 3220\n",
      "39\n",
      "get page 334, length of df: 3230\n",
      "39\n",
      "get page 335, length of df: 3240\n",
      "39\n",
      "get page 336, length of df: 3250\n",
      "39\n",
      "get page 337, length of df: 3260\n",
      "39\n",
      "get page 338, length of df: 3270\n",
      "39\n",
      "get page 339, length of df: 3280\n",
      "39\n",
      "get page 340, length of df: 3290\n",
      "39\n",
      "get page 341, length of df: 3300\n",
      "39\n",
      "get page 342, length of df: 3310\n",
      "38\n",
      "get page 343, length of df: 3320\n",
      "39\n",
      "get page 344, length of df: 3330\n",
      "39\n",
      "get page 345, length of df: 3340\n",
      "39\n",
      "get page 346, length of df: 3350\n",
      "39\n",
      "get page 347, length of df: 3360\n",
      "39\n",
      "get page 348, length of df: 3370\n",
      "38\n",
      "get page 349, length of df: 3380\n",
      "39\n",
      "get page 350, length of df: 3390\n",
      "39\n",
      "get page 351, length of df: 3400\n",
      "39\n",
      "get page 352, length of df: 3410\n",
      "39\n",
      "get page 353, length of df: 3420\n",
      "39\n",
      "get page 354, length of df: 3430\n",
      "39\n",
      "get page 355, length of df: 3440\n",
      "38\n",
      "get page 356, length of df: 3450\n",
      "39\n",
      "get page 357, length of df: 3460\n",
      "39\n",
      "get page 358, length of df: 3470\n",
      "38\n",
      "get page 359, length of df: 3480\n",
      "39\n",
      "get page 360, length of df: 3490\n",
      "39\n",
      "get page 361, length of df: 3500\n",
      "39\n",
      "get page 362, length of df: 3510\n",
      "39\n",
      "get page 363, length of df: 3520\n",
      "39\n",
      "get page 364, length of df: 3530\n",
      "39\n",
      "get page 365, length of df: 3540\n",
      "39\n",
      "get page 366, length of df: 3550\n",
      "38\n",
      "get page 367, length of df: 3560\n",
      "39\n",
      "get page 368, length of df: 3570\n",
      "39\n",
      "get page 369, length of df: 3580\n",
      "39\n",
      "get page 370, length of df: 3590\n",
      "39\n",
      "get page 371, length of df: 3600\n",
      "39\n",
      "get page 372, length of df: 3610\n",
      "38\n",
      "get page 373, length of df: 3620\n",
      "39\n",
      "get page 374, length of df: 3630\n",
      "39\n",
      "get page 375, length of df: 3640\n",
      "39\n",
      "get page 376, length of df: 3650\n",
      "39\n",
      "get page 377, length of df: 3660\n",
      "38\n",
      "get page 378, length of df: 3670\n",
      "39\n",
      "get page 379, length of df: 3680\n",
      "0\n",
      "get page 380, length of df: 3680\n",
      "39\n",
      "get page 381, length of df: 3690\n",
      "39\n",
      "get page 382, length of df: 3700\n",
      "39\n",
      "get page 383, length of df: 3710\n",
      "39\n",
      "get page 384, length of df: 3720\n",
      "39\n",
      "get page 385, length of df: 3730\n",
      "39\n",
      "get page 386, length of df: 3740\n",
      "39\n",
      "get page 387, length of df: 3750\n",
      "39\n",
      "get page 388, length of df: 3760\n",
      "39\n",
      "get page 389, length of df: 3770\n",
      "39\n",
      "get page 390, length of df: 3780\n",
      "39\n",
      "get page 391, length of df: 3790\n",
      "37\n",
      "get page 392, length of df: 3800\n",
      "39\n",
      "get page 393, length of df: 3810\n",
      "39\n",
      "get page 394, length of df: 3820\n",
      "39\n",
      "get page 395, length of df: 3830\n",
      "39\n",
      "get page 396, length of df: 3840\n",
      "39\n",
      "get page 397, length of df: 3850\n",
      "0\n",
      "get page 398, length of df: 3850\n",
      "39\n",
      "get page 399, length of df: 3860\n",
      "39\n",
      "get page 400, length of df: 3870\n",
      "38\n",
      "get page 401, length of df: 3880\n",
      "39\n",
      "get page 402, length of df: 3890\n",
      "39\n",
      "get page 403, length of df: 3900\n",
      "39\n",
      "get page 404, length of df: 3910\n",
      "39\n",
      "get page 405, length of df: 3920\n",
      "39\n",
      "get page 406, length of df: 3930\n",
      "39\n",
      "get page 407, length of df: 3940\n",
      "39\n",
      "get page 408, length of df: 3950\n",
      "39\n",
      "get page 409, length of df: 3960\n",
      "39\n",
      "get page 410, length of df: 3970\n",
      "0\n",
      "get page 411, length of df: 3970\n",
      "39\n",
      "get page 412, length of df: 3980\n",
      "39\n",
      "get page 413, length of df: 3990\n",
      "39\n",
      "get page 414, length of df: 4000\n",
      "39\n",
      "get page 415, length of df: 4010\n",
      "38\n",
      "get page 416, length of df: 4020\n",
      "39\n",
      "get page 417, length of df: 4030\n",
      "39\n",
      "get page 418, length of df: 4040\n",
      "39\n",
      "get page 419, length of df: 4050\n",
      "38\n",
      "get page 420, length of df: 4060\n",
      "39\n",
      "get page 421, length of df: 4070\n",
      "39\n",
      "get page 422, length of df: 4080\n",
      "39\n",
      "get page 423, length of df: 4090\n",
      "39\n",
      "get page 424, length of df: 4100\n",
      "39\n",
      "get page 425, length of df: 4110\n",
      "39\n",
      "get page 426, length of df: 4120\n",
      "39\n",
      "get page 427, length of df: 4130\n",
      "39\n",
      "get page 428, length of df: 4140\n",
      "39\n",
      "get page 429, length of df: 4150\n",
      "36\n",
      "get page 430, length of df: 4160\n",
      "39\n",
      "get page 431, length of df: 4170\n",
      "38\n",
      "get page 432, length of df: 4180\n",
      "38\n",
      "get page 433, length of df: 4190\n",
      "39\n",
      "get page 434, length of df: 4200\n",
      "37\n",
      "get page 435, length of df: 4210\n",
      "38\n",
      "get page 436, length of df: 4220\n",
      "38\n",
      "get page 437, length of df: 4230\n",
      "39\n",
      "get page 438, length of df: 4240\n",
      "39\n",
      "get page 439, length of df: 4250\n",
      "39\n",
      "get page 440, length of df: 4260\n",
      "38\n",
      "get page 441, length of df: 4270\n",
      "37\n",
      "get page 442, length of df: 4280\n",
      "39\n",
      "get page 443, length of df: 4290\n",
      "38\n",
      "get page 444, length of df: 4300\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,445):\n",
    "    my_url = 'https://www.amazon.com/Fire-Fury-Inside-Trump-White/' +\\\n",
    "         'product-reviews/1250158060/ref=cm_cr_getr_d_paging_btm_' + str(i) +\\\n",
    "         '?ie=UTF8&reviewerType=all_reviews&pageNumber=' + str(i)\n",
    "    page = requests.get(my_url)\n",
    "\n",
    "    randtime = np.random.random(345) + np.random.randint(2,8,345)\n",
    "    time.sleep(randtime[i-100])\n",
    "    \n",
    "    html_contents = page.text\n",
    "    soup = BeautifulSoup(html_contents, \"html.parser\")   \n",
    "    \n",
    "    onepage = {'reviewer': get_authers(),\n",
    "           'rating': get_stars(),\n",
    "           'date': get_dates(),\n",
    "           'title': get_titles,\n",
    "           'content': get_revbody()}\n",
    "    try:\n",
    "        df_new = pd.DataFrame.from_dict(onepage)\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "        print(\"get page {}, length of df: {}\".format(i, len(df)))\n",
    "    except ValueError:\n",
    "        print (\"get page {}, arrays must all be same length, length of df: {}\".format(i, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T09:19:42.249543Z",
     "start_time": "2018-02-05T09:19:42.163921Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('complete_tillFeb5.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T07:32:02.650985Z",
     "start_time": "2018-02-05T07:32:02.598032Z"
    }
   },
   "source": [
    "df.to_csv('partial_review_to200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T07:42:47.588329Z",
     "start_time": "2018-02-05T07:33:22.525201Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(200,300):\n",
    "    my_url = 'https://www.amazon.com/Fire-Fury-Inside-Trump-White/' +\\\n",
    "         'product-reviews/1250158060/ref=cm_cr_getr_d_paging_btm_' + str(i) +\\\n",
    "         '?ie=UTF8&reviewerType=all_reviews&pageNumber=' + str(i)\n",
    "    page = requests.get(my_url)\n",
    "\n",
    "    randtime = np.random.random(100) + np.random.randint(2,8,100)\n",
    "    time.sleep(randtime[i-200])\n",
    "    \n",
    "    html_contents = page.text\n",
    "    soup = BeautifulSoup(html_contents, \"html.parser\")   \n",
    "    \n",
    "    onepage = {'reviewer': get_authers(),\n",
    "           'rating': get_stars(),\n",
    "           'date': get_dates(),\n",
    "           'title': get_titles,\n",
    "           'content': get_revbody()}\n",
    "    try:\n",
    "        df_new = pd.DataFrame.from_dict(onepage)\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "        print(\"get page {}, length of df: {}\".format(i, len(df)))\n",
    "    except ValueError:\n",
    "        print (\"get page {}, arrays must all be same length, length of df: {}\".format(i, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T07:46:13.010051Z",
     "start_time": "2018-02-05T07:46:12.949868Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('partial_review_to300.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:00:23.647876Z",
     "start_time": "2018-02-05T07:46:35.311645Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(300,445):\n",
    "    my_url = 'https://www.amazon.com/Fire-Fury-Inside-Trump-White/' +\\\n",
    "         'product-reviews/1250158060/ref=cm_cr_getr_d_paging_btm_' + str(i) +\\\n",
    "         '?ie=UTF8&reviewerType=all_reviews&pageNumber=' + str(i)\n",
    "    page = requests.get(my_url)\n",
    "\n",
    "    randtime = np.random.random(145) + np.random.randint(2,8,145)\n",
    "    time.sleep(randtime[i-300])\n",
    "    \n",
    "    html_contents = page.text\n",
    "    soup = BeautifulSoup(html_contents, \"html.parser\")   \n",
    "    \n",
    "    onepage = {'reviewer': get_authers(),\n",
    "           'rating': get_stars(),\n",
    "           'date': get_dates(),\n",
    "           'title': get_titles,\n",
    "           'content': get_revbody()}\n",
    "    try:\n",
    "        df_new = pd.DataFrame.from_dict(onepage)\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "        print(\"get page {}, length of df: {}\".format(i, len(df)))\n",
    "    except ValueError:\n",
    "        print (\"get page {}, arrays must all be same length, length of df: {}\".format(i, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:00:36.629606Z",
     "start_time": "2018-02-05T08:00:36.557112Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('partial_review_to445.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:00:42.074278Z",
     "start_time": "2018-02-05T08:00:42.068537Z"
    }
   },
   "outputs": [],
   "source": [
    "pagelist = [152, 160, 168, 200, 201, 202, 220, 221, 226, 234, 235,\n",
    "            247, 263, 266, 271, 272, 277, 281, 284, 285, 287, 297,\n",
    "            309, 319, 322, 323, 324, 325, 343, 348, 355, 359, 366,\n",
    "            373, 378, 380, 392, 401, 416, 420, 429, 430, 432, 433,\n",
    "            435, 436, 437, 441, 442, 444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:08:51.752113Z",
     "start_time": "2018-02-05T08:03:45.363541Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,num in enumerate(pagelist):\n",
    "    my_url = 'https://www.amazon.com/Fire-Fury-Inside-Trump-White/' +\\\n",
    "         'product-reviews/1250158060/ref=cm_cr_getr_d_paging_btm_' + str(num) +\\\n",
    "         '?ie=UTF8&reviewerType=all_reviews&pageNumber=' + str(num)\n",
    "    page = requests.get(my_url)\n",
    "\n",
    "    randtime = np.random.random(len(pagelist)) + np.random.randint(2,8,len(pagelist))\n",
    "    time.sleep(randtime[i])\n",
    "    \n",
    "    html_contents = page.text\n",
    "    soup = BeautifulSoup(html_contents, \"html.parser\")   \n",
    "    \n",
    "    onepage = {'reviewer': get_authers(),\n",
    "           'rating': get_stars(),\n",
    "           'date': get_dates(),\n",
    "           'title': get_titles,\n",
    "           'content': get_revbody()}\n",
    "    try:\n",
    "        df_new = pd.DataFrame.from_dict(onepage)\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "        print(\"get page {}, length of df: {}\".format(i, len(df)))\n",
    "    except ValueError:\n",
    "        print (\"get page {}, arrays must all be same length, length of df: {}\".format(i, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T08:12:05.130425Z",
     "start_time": "2018-02-05T08:12:05.040289Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('review_tillFeb5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
